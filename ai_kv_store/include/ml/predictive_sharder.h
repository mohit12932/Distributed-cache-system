// ai_kv_store/include/ml/predictive_sharder.h
// ────────────────────────────────────────────────────────────────
// Predictive Sharder: monitors traffic telemetry, trains the PINN,
// and triggers proactive shard migrations when predicted pressure
// exceeds threshold N.
// ────────────────────────────────────────────────────────────────
#pragma once

#include <algorithm>
#include <atomic>
#include <chrono>
#include <cstdint>
#include <deque>
#include <functional>
#include <mutex>
#include <random>
#include <thread>
#include <vector>

#include "absl/synchronization/mutex.h"
#include "absl/time/clock.h"

#include "pinn_model.h"

namespace ai_kv {
namespace ml {

// ── Traffic telemetry sample ──

struct TelemetrySample {
    double   timestamp;    // Seconds since epoch
    uint32_t shard_id;
    double   qps;          // Queries per second on this shard
    double   p99_latency;  // Microseconds
    double   write_ratio;  // Fraction of writes
};

// ── Migration request (generated by the sharder) ──

struct MigrationRequest {
    uint32_t source_shard;
    uint32_t target_shard;
    double   predicted_heat_source;
    double   predicted_heat_target;
    double   prediction_timestamp;
};

// ── Ring buffer for per-shard telemetry ──

class TelemetryRingBuffer {
public:
    explicit TelemetryRingBuffer(size_t capacity = 4096)
        : capacity_(capacity) {}

    void Push(const TelemetrySample& sample) {
        absl::MutexLock lock(&mu_);
        if (buffer_.size() >= capacity_) {
            buffer_.pop_front();
        }
        buffer_.push_back(sample);
    }

    // Get samples within a time window [t_start, t_end]
    std::vector<TelemetrySample> GetWindow(double t_start, double t_end) const {
        absl::ReaderMutexLock lock(&mu_);
        std::vector<TelemetrySample> result;
        for (const auto& s : buffer_) {
            if (s.timestamp >= t_start && s.timestamp <= t_end) {
                result.push_back(s);
            }
        }
        return result;
    }

    size_t Size() const {
        absl::ReaderMutexLock lock(&mu_);
        return buffer_.size();
    }

private:
    mutable absl::Mutex             mu_;
    std::deque<TelemetrySample>     buffer_ ABSL_GUARDED_BY(mu_);
    size_t                          capacity_;
};

// ── Predictive Sharder ──

class PredictiveSharder {
public:
    using MigrationCallback = std::function<void(const MigrationRequest&)>;

    struct Config {
        int    num_shards          = 8;
        double pressure_threshold  = 0.8;   // Normalized heat threshold N
        double prediction_horizon  = 10.0;  // Seconds ahead to predict
        double train_interval      = 5.0;   // Seconds between retraining
        double eval_interval       = 1.0;   // Seconds between migration checks
        int    train_iterations    = 200;    // Adam iterations per retrain
        int    collocation_points  = 256;    // PDE residual sample count
        double window_duration     = 60.0;  // Training data window (seconds)
        float  viscosity           = 0.01f;
    };

    explicit PredictiveSharder(const Config& config = Config{})
        : config_(config),
          telemetry_(8192),
          shutdown_(false) {
        // Configure PINN
        PINNConfig pinn_cfg;
        pinn_cfg.num_shards = config_.num_shards;
        pinn_cfg.viscosity  = config_.viscosity;
        pinn_model_ = std::make_unique<DoubleBufferedPINN>(pinn_cfg);
    }

    ~PredictiveSharder() { Stop(); }

    // ── Lifecycle ──

    void Start() {
        shutdown_.store(false);
        trainer_thread_ = std::thread([this] { TrainerLoop(); });
        eval_thread_    = std::thread([this] { EvalLoop(); });
    }

    void Stop() {
        shutdown_.store(true, std::memory_order_release);
        if (trainer_thread_.joinable()) trainer_thread_.join();
        if (eval_thread_.joinable()) eval_thread_.join();
    }

    // ── Telemetry ingestion (called by request handlers) ──

    void RecordSample(const TelemetrySample& sample) {
        telemetry_.Push(sample);
    }

    // Convenience: record a single operation
    void RecordOperation(uint32_t shard_id, double latency_us, bool is_write) {
        auto now = std::chrono::duration<double>(
            std::chrono::steady_clock::now().time_since_epoch()).count();

        // Accumulate into per-shard counters for QPS computation
        absl::MutexLock lock(&counter_mu_);
        auto& c = shard_counters_[shard_id];
        c.ops++;
        c.latency_sum += latency_us;
        if (is_write) c.writes++;
    }

    // ── Set migration callback ──

    void SetMigrationCallback(MigrationCallback cb) {
        migration_cb_ = std::move(cb);
    }

    // ── Query current heat map ──

    std::vector<PINNModel::ShardPrediction> GetCurrentHeatMap() const {
        auto model = pinn_model_->ActiveModel();
        auto now = NormalizedTime();
        return model->PredictHeatMap(now,
                   static_cast<float>(config_.prediction_horizon));
    }

private:
    // ── Trainer loop: periodically retrain the PINN ──

    void TrainerLoop() {
        while (!shutdown_.load(std::memory_order_acquire)) {
            auto start = std::chrono::steady_clock::now();

            RetrainModel();

            // Sleep until next training interval
            auto elapsed = std::chrono::steady_clock::now() - start;
            auto sleep_ms = std::chrono::milliseconds(
                static_cast<int>(config_.train_interval * 1000)) - elapsed;
            if (sleep_ms.count() > 0) {
                std::this_thread::sleep_for(sleep_ms);
            }
        }
    }

    // ── Eval loop: check PINN predictions and trigger migrations ──

    void EvalLoop() {
        while (!shutdown_.load(std::memory_order_acquire)) {
            std::this_thread::sleep_for(
                std::chrono::milliseconds(
                    static_cast<int>(config_.eval_interval * 1000)));

            EvaluateAndMigrate();
        }
    }

    // ── Core training logic ──

    void RetrainModel() {
        auto now = NormalizedTime();
        float window = static_cast<float>(config_.window_duration);

        // Collect training data from telemetry ring buffer
        double t_start = now - window;
        auto samples = telemetry_.GetWindow(t_start, now);
        if (samples.size() < 10) return;  // Not enough data

        // Normalize QPS into [0, 1] range
        double max_qps = 1.0;
        for (const auto& s : samples) {
            max_qps = std::max(max_qps, s.qps);
        }

        // Build data points
        std::vector<TrafficSample> data_points;
        for (const auto& s : samples) {
            data_points.push_back({
                static_cast<float>((s.timestamp - t_start) / window),  // Normalize t to [0, 1]
                static_cast<float>(s.shard_id),
                static_cast<float>(s.qps / max_qps)
            });
        }

        // Generate Latin Hypercube collocation points for PDE residual
        auto collocation = GenerateCollocationPoints(
            config_.collocation_points, window);

        // Initial condition points (earliest samples)
        std::vector<TrafficSample> ic_points;
        for (const auto& dp : data_points) {
            if (dp.t < 0.05f) {  // First 5% of window
                ic_points.push_back(dp);
            }
        }

        // Get the inactive model and train it
        PINNModel* model = pinn_model_->InactiveModel();

        // Copy current active weights as warm start
        auto active = pinn_model_->ActiveModel();
        model->LoadSnapshot(active->TakeSnapshot());

        // Train for N iterations
        for (int i = 0; i < config_.train_iterations; ++i) {
            model->TrainStep(data_points, collocation, ic_points);
        }

        // Atomic swap: make newly trained model active
        pinn_model_->SwapModels();
    }

    // ── Migration evaluation ──

    void EvaluateAndMigrate() {
        auto model = pinn_model_->ActiveModel();
        float t_now = NormalizedTime();
        float t_future = t_now + static_cast<float>(config_.prediction_horizon);

        auto predictions = model->PredictHeatMap(t_now,
                               static_cast<float>(config_.prediction_horizon));

        // Sort by predicted heat
        std::vector<size_t> indices(predictions.size());
        for (size_t i = 0; i < indices.size(); ++i) indices[i] = i;

        std::sort(indices.begin(), indices.end(),
                  [&](size_t a, size_t b) {
                      return predictions[a].predicted_heat >
                             predictions[b].predicted_heat;
                  });

        // Check hot shards against threshold
        size_t cool_idx = indices.size() - 1;
        for (size_t i = 0; i < indices.size() && predictions[indices[i]].predicted_heat >
                    config_.pressure_threshold; ++i) {

            if (cool_idx <= i) break;  // No cool shards left

            auto& hot  = predictions[indices[i]];
            auto& cool = predictions[indices[cool_idx]];

            if (migration_cb_) {
                MigrationRequest req;
                req.source_shard          = hot.shard_id;
                req.target_shard          = cool.shard_id;
                req.predicted_heat_source = hot.predicted_heat;
                req.predicted_heat_target = cool.predicted_heat;
                req.prediction_timestamp  = static_cast<double>(t_future);
                migration_cb_(req);
            }

            --cool_idx;
        }
    }

    // ── Helpers ──

    std::vector<std::pair<float, float>> GenerateCollocationPoints(
            int n, float window) const {
        // Latin Hypercube Sampling over [0, 1] × [0, S]
        std::mt19937 rng(42 + pinn_model_->ActiveModel()->TrainStepCount());
        std::vector<std::pair<float, float>> points;
        points.reserve(n);

        float S = static_cast<float>(config_.num_shards);
        for (int i = 0; i < n; ++i) {
            float t = (static_cast<float>(i) + Rand01(rng)) / static_cast<float>(n);
            float x = Rand01(rng) * S;
            points.emplace_back(t, x);
        }

        // Shuffle for de-correlation
        std::shuffle(points.begin(), points.end(), rng);
        return points;
    }

    static float Rand01(std::mt19937& rng) {
        return std::uniform_real_distribution<float>(0.0f, 1.0f)(rng);
    }

    static float NormalizedTime() {
        return static_cast<float>(
            std::chrono::duration<double>(
                std::chrono::steady_clock::now().time_since_epoch()).count());
    }

    // Flush per-shard counters into telemetry ring buffer
    void FlushCounters() {
        absl::MutexLock lock(&counter_mu_);
        auto now = NormalizedTime();
        for (auto& kv : shard_counters_) {
            if (kv.second.ops > 0) {
                TelemetrySample s;
                s.timestamp   = now;
                s.shard_id    = kv.first;
                s.qps         = static_cast<double>(kv.second.ops);
                s.p99_latency = kv.second.ops > 0
                    ? kv.second.latency_sum / kv.second.ops : 0;
                s.write_ratio = kv.second.ops > 0
                    ? static_cast<double>(kv.second.writes) / kv.second.ops
                    : 0;
                telemetry_.Push(s);
            }
            kv.second = {};
        }
    }

    struct ShardCounter {
        uint64_t ops         = 0;
        double   latency_sum = 0;
        uint64_t writes      = 0;
    };

    // ── Fields ──

    Config                                          config_;
    TelemetryRingBuffer                             telemetry_;
    std::unique_ptr<DoubleBufferedPINN>             pinn_model_;

    absl::Mutex                                     counter_mu_;
    std::unordered_map<uint32_t, ShardCounter>      shard_counters_
                                                        ABSL_GUARDED_BY(counter_mu_);

    MigrationCallback                               migration_cb_;

    std::thread                                     trainer_thread_;
    std::thread                                     eval_thread_;
    std::atomic<bool>                               shutdown_;
};

}  // namespace ml
}  // namespace ai_kv
